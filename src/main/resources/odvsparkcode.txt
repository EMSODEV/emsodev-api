import java.sql.Timestamp\r\nimport scala.collection.mutable.ListBuffer\r\nimport java.time.LocalDateTime\r\nimport java.time.ZoneId\r\nimport java.util.UUID\r\nimport org.apache.spark.sql.Row\r\n\r\nval EGIMNode = \"egimNodeVar\"\r\nval InstrumentID = \"instrumentIdVar\"\r\n\r\nval startDate = LocalDateTime.of(startDateVar)\r\nval utcTzSD = ZoneId.of(\"UTC\")\r\nstartDate.atZone(utcTzSD)\r\nval endDate = LocalDateTime.of(endDateVar)\r\nval utcTzED = ZoneId.of(\"UTC\")\r\nendDate.atZone(utcTzED)\r\n\r\nval df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"false\").option(\"delimiter\",\"\\t\").load(\"\/emsodev\/\" + EGIMNode + \"\/\" + InstrumentID + \"\/*\/insertResult\/*\")\r\n\r\nval dfInTimeRange = df.where((unix_timestamp(df(\"C0\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\")).cast(\"timestamp\").between(Timestamp.valueOf(startDate), Timestamp.valueOf(endDate)))\r\n\r\ndfInTimeRange.registerTempTable(\"dfInTimeRange\");\r\n\r\nval dfInTimeRangeWithCounter = sqlContext.sql(\"SELECT row_number() OVER (order by C0) as Counter,dfInTimeRange.* FROM dfInTimeRange\")\r\n\r\nval df_metadata = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"false\").option(\"delimiter\",\"\\t\").load(\"\/emsodev\/\" + EGIMNode + \"\/\" + InstrumentID + \"\/*\/ODVMetadata.txt\")\r\n    \r\nval df_metadataWithDate = df_metadata.withColumn(\"C3\", when(col(\"C3\").equalTo(\"[SampleStartDate]\"), startDate.toString())).withColumn(\"C7\", when(col(\"C7\").equalTo(\"[EDMO_code]\"), \"3917\"))\r\n\r\nval dfInTimeRangeCountMinus1 = (dfInTimeRange.count() - 1).toInt\r\n\r\nvar tabulateWithTabs = new ListBuffer[(String,String,String,String,String,String,String,String,String)]()\r\n\r\nfor(i <- 1 to dfInTimeRangeCountMinus1){\r\n    \r\n    tabulateWithTabs .+= ((\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"))\r\n   \r\n}\r\n\r\nval multiRowWithTabsDF = sc.parallelize(tabulateWithTabs).toDF()\r\n\r\nval metadataDF = df_metadataWithDate.unionAll(multiRowWithTabsDF)\r\n\r\nmetadataDF.registerTempTable(\"metadataDF\");\r\n\r\nval metadataDFWithCounter = sqlContext.sql(\"SELECT row_number() OVER (order by C0 DESC) as Counter, metadataDF.* FROM metadataDF\").toDF(\"Counter\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\")\r\n\r\nval odvDFWithCounter = metadataDFWithCounter.join(dfInTimeRangeWithCounter, \"Counter\")\r\n\r\nval odvDF = odvDFWithCounter.drop(\"Counter\")\r\n\r\nval df_header = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"false\").option(\"delimiter\", \"\\t\").load(\"\/emsodev\/\" + EGIMNode + \"\/\" + InstrumentID + \"\/*\/ODVHeader.txt\")\r\n\r\nval row_rdd = df_header.rdd.map(x => x.mkString(\",\").replaceAll(\"\\\\\\\\u00ba\", \"\u00B0\").split(',')).map(x => Row.fromSeq(x))\r\n\r\nvar df_headerUpdated = sqlContext.createDataFrame(row_rdd, df_header.schema)\r\n\r\nval uuid = UUID.randomUUID.toString\r\n\r\nval odvDFWithHeader = df_headerUpdated.unionAll(odvDF)\r\n\r\nodvDFWithHeader.rdd.map(x => x.mkString(\"\\t\")).coalesce(1).saveAsTextFile(\"\/emsodev\/ODVFiles\/\" + uuid)\r\n\r\nprintln(uuid)\r\n